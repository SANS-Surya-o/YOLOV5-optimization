{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SANS-Surya-o/YOLOV5-optimization/blob/main/YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlWKIgpzqXIg",
        "outputId": "3f3059f9-0b6a-4e25-d77d-992f25bbd955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17372, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 17372 (delta 42), reused 20 (delta 20), pack-reused 17313 (from 3)\u001b[K\n",
            "Receiving objects: 100% (17372/17372), 16.25 MiB | 17.28 MiB/s, done.\n",
            "Resolving deltas: 100% (11910/11910), done.\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.14.1)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (8.3.107)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "/content/yolov5/yolov5\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchinfo # import torch\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!cd yolov5 && pip install -r requirements.txt\n",
        "%cd yolov5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import yaml\n",
        "import subprocess\n",
        "import re\n",
        "from torchinfo import summary\n",
        "\n",
        "# Add YOLOv5 to path\n",
        "yolov5_path = 'yolov5'\n",
        "sys.path.append(yolov5_path)\n",
        "\n",
        "\n",
        "from yolov5.models.common import DetectMultiBackend\n",
        "from yolov5.utils.torch_utils import select_device\n",
        "from yolov5.utils.general import non_max_suppression, scale_boxes\n",
        "from yolov5.utils.augmentations import letterbox"
      ],
      "metadata": {
        "id": "HHpw8TU_sYM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO Dataset\n",
        "def download_coco_subset():\n",
        "    \"\"\"Download a subset of COCO validation images\"\"\"\n",
        "    os.makedirs('coco_subset', exist_ok=True)\n",
        "\n",
        "    # Download COCO val2017 zip file\n",
        "    !wget -q http://images.cocodataset.org/zips/val2017.zip -O coco_val.zip\n",
        "    !unzip -q coco_val.zip -d temp_coco\n",
        "\n",
        "    # Select first 100 images\n",
        "    import shutil\n",
        "    coco_images = list(Path('temp_coco/val2017').glob('*.jpg'))[:100]\n",
        "    for i, img_path in enumerate(coco_images):\n",
        "        shutil.copy(img_path, f'coco_subset/img_{i:03d}.jpg')\n",
        "\n",
        "    # Clean up\n",
        "    !rm -rf temp_coco coco_val.zip\n",
        "\n",
        "    return 'coco_subset'\n"
      ],
      "metadata": {
        "id": "bvliPs-isYVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = download_coco_subset()\n",
        "# Get all image paths\n",
        "image_paths = list(Path(dataset_path).glob('*.jpg')) + list(Path(dataset_path).glob('*.png'))\n",
        "print(f\"Using {len(image_paths)} images from {dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9xFEQNHuv62",
        "outputId": "98aab162-470b-49bb-bb31-139096be9ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 100 images from coco_subset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv5 Models\n",
        "device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blPq4sYGvSCH",
        "outputId": "4db06244-ca7a-4974-8255-b77e0fd2b526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "3MH8vZDQMWiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_variants = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
        "results = []\n",
        "model_benchmark_results = {}\n",
        "\n",
        "\n",
        "for model in model_variants:\n",
        "  print(f\"Benchmarking {model}\")\n",
        "\n",
        "  # Run detection with timing\n",
        "  start_time = time.time()\n",
        "  cmd = f\"python3 detect.py --weights {model}.pt --source {dataset_path} --device {device} --save-txt\"\n",
        "  result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "  output = result.stderr\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate FPS\n",
        "  fps = len(image_paths) / (end_time - start_time)\n",
        "\n",
        "  # Calculate latency - search for the string in the given format from the output of the command detect.py\n",
        "  match = re.search(r\"Speed:\\s+([\\d.]+)ms pre-process,\\s+([\\d.]+)ms inference,\\s+([\\d.]+)ms NMS\", output)\n",
        "\n",
        "\n",
        "  if match:\n",
        "      pre = float(match.group(1))\n",
        "      infer = float(match.group(2))\n",
        "      nms = float(match.group(3))\n",
        "      total_latency = round(pre + infer + nms, 2)\n",
        "      results.append({\n",
        "          \"model\": model,\n",
        "          \"latency\": total_latency,\n",
        "          \"fps\": fps\n",
        "      })\n",
        "      model_benchmark_results[model] = {\n",
        "          \"latency\": total_latency,\n",
        "          \"fps\": fps\n",
        "      }\n",
        "  else:\n",
        "      print(\"Could not find timing info in output.\")\n",
        "      print(output)\n",
        "\n",
        "\n",
        "# Create performance dataframe\n",
        "benchmark_df = pd.DataFrame(results)\n",
        "print(\"\\nPerformance comparison:\")\n",
        "print(benchmark_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "iPZBbn1kxS5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5c8d4b-c15b-4996-86ab-742e0e07b801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking yolov5n\n",
            "Benchmarking yolov5s\n",
            "Benchmarking yolov5m\n",
            "Benchmarking yolov5l\n",
            "Benchmarking yolov5x\n",
            "\n",
            "Performance comparison:\n",
            "     model  latency        fps\n",
            "0  yolov5n     15.4   9.935050\n",
            "1  yolov5s     16.2  10.070236\n",
            "2  yolov5m     21.6   9.407299\n",
            "3  yolov5l     24.0   8.738181\n",
            "4  yolov5x     27.3   8.572814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "ri9Q-iqSLgAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_stats = []\n",
        "\n",
        "# Used T4 GPU on google colab which has peak gflops of 8100\n",
        "gpu_peak_gflops = 8100\n",
        "\n",
        "for model_name in model_variants:\n",
        "    # Load the model\n",
        "    model = torch.hub.load('ultralytics/yolov5', model_name, pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # Get model size in MB\n",
        "    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "\n",
        "    # Get param count\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Use torchinfo to get stats\n",
        "    stats = summary(model, input_size=(1, 3, 640, 640), verbose=0)\n",
        "\n",
        "    # Extract FLOPS (multiply-accumulates)\n",
        "    # For YOLOv5, each MAC operation is ~2 FLOPS\n",
        "    macs = stats.total_mult_adds\n",
        "    gflops = macs * 2 / 1e9\n",
        "\n",
        "    latency_ms = benchmark_df[benchmark_df['model'] == model_name]['latency'].values[0]\n",
        "    actual_gflops_per_sec = gflops / (latency_ms / 1000)\n",
        "\n",
        "    utilization = (actual_gflops_per_sec / gpu_peak_gflops)\n",
        "\n",
        "    if utilization == 1:\n",
        "      bound_type = \"compute\"\n",
        "    elif utilization < 1:\n",
        "      bound_type = \"memory\"\n",
        "    else:\n",
        "      bound_type = \"none\"\n",
        "\n",
        "    model_stats.append({\n",
        "        'Model': model_name,\n",
        "        'Parameters (M)': param_count / 1e6,\n",
        "        'Model Size (MB)': model_size_mb,\n",
        "        'GFLOPS per inference': gflops,\n",
        "        'GFLOPS/sec': actual_gflops_per_sec,\n",
        "        'Utilization (%)': utilization,\n",
        "        'Bound Type': bound_type\n",
        "    })\n",
        "\n",
        "\n",
        "model_stats_df = pd.DataFrame(model_stats)\n",
        "model_stats_df = model_stats_df.round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3QhBn3OMgIu",
        "outputId": "f8078efd-ce44-460b-fe53-a713ba8baaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(\"The GPU used has a peak flop performance of 8.1 TFLOPS/s - (T4 GPU on google colab)\")\n",
        "print(model_stats_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfpgLldYViIC",
        "outputId": "8828fb82-7b51-451e-ade0-711a2f949d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The GPU used has a peak flop performance of 8.1 TFLOPS/s - (T4 GPU on google colab)\n",
            "     Model  Parameters (M)  Model Size (MB)  GFLOPS per inference  GFLOPS/sec  \\\n",
            "0  yolov5n            1.87             7.12                  4.50      223.68   \n",
            "1  yolov5s            7.23            27.56                 16.49     1084.55   \n",
            "2  yolov5m           21.17            80.77                 48.97     2766.52   \n",
            "3  yolov5l           46.53           177.51                109.15     3816.26   \n",
            "4  yolov5x           86.71           330.75                205.67     4932.11   \n",
            "\n",
            "   Utilization (%) Bound Type  \n",
            "0             0.03     memory  \n",
            "1             0.13     memory  \n",
            "2             0.34     memory  \n",
            "3             0.47     memory  \n",
            "4             0.61     memory  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer wise analysis"
      ],
      "metadata": {
        "id": "0Wz8EyR-CS9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore.nn"
      ],
      "metadata": {
        "id": "RSdhzuRLCXCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "def analyze_model_layers(model_name):\n",
        "    model = torch.hub.load('ultralytics/yolov5', model_name, pretrained=True, device='cpu')\n",
        "    model.eval()\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Create sample input on CPU\n",
        "    sample_input = torch.rand(1, 3, 640, 640, device='cpu')\n",
        "\n",
        "    # Use fvcore for FLOP counting\n",
        "    flop_counter = FlopCountAnalysis(model, sample_input)\n",
        "\n",
        "    # Get total FLOPs\n",
        "    total_flops = flop_counter.total()\n",
        "\n",
        "    # Get per-module FLOP counts\n",
        "    module_flops = flop_counter.by_module()\n",
        "\n",
        "    # Get per-module-and-operator FLOP counts for more detailed analysis\n",
        "    module_op_flops = flop_counter.by_module_and_operator()\n",
        "\n",
        "    # Convert to a more analyzable format\n",
        "    layer_data = []\n",
        "    for module_name, flops in module_flops.items():\n",
        "        # Skip the empty module name (represents the whole model)\n",
        "        if module_name == '':\n",
        "            continue\n",
        "\n",
        "        # Get the module's percentage of total computation\n",
        "        percentage = (flops / total_flops) * 100\n",
        "\n",
        "        # Get operator breakdown if available\n",
        "        op_breakdown = module_op_flops.get(module_name, {})\n",
        "\n",
        "        layer_data.append({\n",
        "            'Layer': module_name,\n",
        "            'FLOPs': flops,\n",
        "            'GFLOPs': flops / 1e9,\n",
        "            'Percentage': percentage,\n",
        "            'Op_Breakdown': op_breakdown,\n",
        "        })\n",
        "\n",
        "    # Sort by FLOPs in descending order\n",
        "    layer_data = sorted(layer_data, key=lambda x: x['FLOPs'], reverse=True)\n",
        "\n",
        "    # Measure CPU inference time\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):  # Run multiple times for more stable measurement\n",
        "            _ = model(sample_input)\n",
        "    end_time = time.time()\n",
        "    inference_time = (end_time - start_time) / 5  # Average time per inference\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'total_flops': total_flops,\n",
        "        'total_gflops': total_flops / 1e9,\n",
        "        'layer_data': layer_data,\n",
        "        'inference_time': inference_time,\n",
        "        'params': params,\n",
        "    }\n",
        "\n",
        "def print_top_layers(analysis_result, max_gflops):\n",
        "    \"\"\"\n",
        "    Print details for all layers in the model (not just top N)\n",
        "    \"\"\"\n",
        "    model_name = analysis_result['model_name']\n",
        "    total_gflops = analysis_result['total_gflops']\n",
        "    layer_data = analysis_result['layer_data']\n",
        "    inference_time = analysis_result['inference_time']\n",
        "\n",
        "    print(f\"\\n==== {model_name} Model Analysis (CPU) ====\")\n",
        "    print(f\"Total GFLOPs: {total_gflops:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time*1000:.2f} ms\")\n",
        "    print(f\"Throughput: {1/inference_time:.2f} FPS\")\n",
        "    print(f\"Average Performance: {total_gflops/inference_time:.2f} GFLOPS/s\")\n",
        "    print(f\"Utilization : {(total_gflops / (max_gflops*inference_time)) * 100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nAll Layers Details:\")\n",
        "    print(\"-\" * 120)\n",
        "    print(f\"{'Rank':<5} {'Layer':<40} {'GFLOPs':<12} {'Percentage':<10} {'Time(ms)':<12} {'GFLOPS/s':<12} {'Utilization':<12}\")\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    layer_data = sorted(layer_data, key=lambda x: x.get('Utilization', 0), reverse=True)\n",
        "\n",
        "    for i, layer in enumerate(layer_data):\n",
        "        utilization = layer.get('Utilization', 0)\n",
        "        measured_time = layer.get('Measured_Time_ms', 0)\n",
        "        gflops_per_second = layer.get('GFLOPS_per_second', 0)\n",
        "\n",
        "        print(f\"{i+1:<5} {layer['Layer']:<40} {layer['GFLOPs']:<12.4f} {layer['Percentage']:<10.2f}% \"\n",
        "              f\"{measured_time:<12.2f} {gflops_per_second:<12.2f} {utilization:<12.2f}%\")\n",
        "\n",
        "def compute_utilization(analysis_result, peak_gflops):\n",
        "    \"\"\"\n",
        "    Compute utilization for each layer based on direct inference time measurements\n",
        "    \"\"\"\n",
        "    layer_data = analysis_result['layer_data']\n",
        "    model = torch.hub.load('ultralytics/yolov5', analysis_result['model_name'], pretrained=True, device='cpu')\n",
        "    model.eval()\n",
        "\n",
        "    # Create sample input on CPU\n",
        "    sample_input = torch.rand(1, 3, 640, 640, device='cpu')\n",
        "\n",
        "    # Define a hook to measure layer execution time\n",
        "    layer_times = {}\n",
        "\n",
        "    def measure_time_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            if name not in layer_times:\n",
        "                layer_times[name] = []\n",
        "            layer_times[name].append(time.time())\n",
        "            return None\n",
        "        return hook\n",
        "\n",
        "    # Register hooks for all modules\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        if name != '':  # Skip the empty module name (represents the whole model)\n",
        "            pre_hook = module.register_forward_pre_hook(\n",
        "                lambda m, inp, name=name: layer_times.setdefault(name, []).append(time.time())\n",
        "            )\n",
        "            post_hook = module.register_forward_hook(measure_time_hook(name))\n",
        "            hooks.append(pre_hook)\n",
        "            hooks.append(post_hook)\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        _ = model(sample_input)\n",
        "\n",
        "    # Clear times from warm-up\n",
        "    layer_times.clear()\n",
        "\n",
        "    # Actual measurement runs\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):  # Run multiple times for more stable measurement\n",
        "            _ = model(sample_input)\n",
        "\n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Calculate average execution time for each layer\n",
        "    layer_execution_times = {}\n",
        "    for name, timestamps in layer_times.items():\n",
        "        # Each layer should have pairs of timestamps (start and end)\n",
        "        if len(timestamps) % 2 == 0:\n",
        "            start_times = timestamps[::2]  # Every other element starting from 0\n",
        "            end_times = timestamps[1::2]   # Every other element starting from 1\n",
        "\n",
        "            # Calculate time differences and average\n",
        "            time_diffs = [end - start for start, end in zip(start_times, end_times)]\n",
        "            layer_execution_times[name] = sum(time_diffs) / len(time_diffs)\n",
        "\n",
        "    # Update layer data with measured times\n",
        "    for layer in layer_data:\n",
        "        layer_name = layer['Layer']\n",
        "        if layer_name in layer_execution_times:\n",
        "            layer_time = layer_execution_times[layer_name]\n",
        "\n",
        "            # Calculate actual GFLOPS/s for this layer\n",
        "            if layer_time > 0:\n",
        "                actual_gflops_per_second = layer['GFLOPs'] / layer_time\n",
        "                # Calculate utilization percentage\n",
        "                utilization = (actual_gflops_per_second / peak_gflops) * 100\n",
        "            else:\n",
        "                actual_gflops_per_second = 0\n",
        "                utilization = 0\n",
        "\n",
        "            # Add to the layer data\n",
        "            layer['Measured_Time_ms'] = layer_time * 1000\n",
        "            layer['GFLOPS_per_second'] = actual_gflops_per_second\n",
        "            layer['Utilization'] = utilization\n",
        "        else:\n",
        "            # If layer not found in measurements, use zeros\n",
        "            layer['Measured_Time_ms'] = 0\n",
        "            layer['GFLOPS_per_second'] = 0\n",
        "            layer['Utilization'] = 0\n",
        "\n",
        "    return analysis_result\n",
        "\n",
        "def run():\n",
        "    # Estimate peak CPU performance\n",
        "    print(\"Estimating peak CPU performance...\")\n",
        "    peak_gflops = 300  # Example value, adjust based on your CPU\n",
        "    print(f\"Estimated peak CPU performance: {peak_gflops:.2f} GFLOPS\")\n",
        "\n",
        "    # Analyze different YOLOv5 variants\n",
        "    model_variants = ['yolov5x']\n",
        "    analysis_results = []\n",
        "\n",
        "    for variant in model_variants:\n",
        "        print(f\"Analyzing {variant} on CPU...\")\n",
        "        result = analyze_model_layers(variant)\n",
        "\n",
        "        # Compute utilization based on measured inference time and estimated peak performance\n",
        "        result = compute_utilization(result, peak_gflops)\n",
        "\n",
        "        analysis_results.append(result)\n",
        "        print_top_layers(result, peak_gflops)\n",
        "\n",
        "run()"
      ],
      "metadata": {
        "id": "9QOcoNtFCVmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "8G43A8FSWM95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch.profiler (used for CPU / GPU split)"
      ],
      "metadata": {
        "id": "tnXmJP_1srse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "dummy_input = torch.randn((1, 3, 640, 640)).to(device)\n",
        "# model = DetectMultiBackend('yolov5s.pt', device=device).to(device).eval()\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device).eval()\n",
        "\n",
        "\n",
        "with profile(\n",
        "    activities=[\n",
        "        ProfilerActivity.CPU,\n",
        "        ProfilerActivity.CUDA] if torch.cuda.is_available() else [ProfilerActivity.CPU],\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        with record_function(\"model_inference\"):\n",
        "            model(dummy_input)\n",
        "\n",
        "# Get profiler output as string\n",
        "prof_output = prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=25)\n",
        "\n",
        "print(prof_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZTMHX8XaP34",
        "outputId": "02dc4d89-a0a0-4cf5-f7fb-a17b1f39d1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        model_inference        42.32%      13.154ms        99.94%      31.064ms      31.064ms       0.000us         0.00%      11.566ms      11.566ms           0 b           0 b     -92.50 Kb    -195.08 Mb             1  \n",
            "                                           aten::conv2d         0.98%     305.809us        29.60%       9.201ms     153.347us       0.000us         0.00%       9.770ms     162.839us           0 b           0 b     106.63 Mb           0 b            60  \n",
            "                                      aten::convolution         0.89%     276.541us        28.62%       8.895ms     148.250us       0.000us         0.00%       9.770ms     162.839us           0 b           0 b     106.63 Mb           0 b            60  \n",
            "                                     aten::_convolution         4.69%       1.458ms        27.73%       8.618ms     143.641us       0.000us         0.00%       9.770ms     162.839us           0 b           0 b     106.63 Mb           0 b            60  \n",
            "                                           aten::arange         9.36%       2.909ms        19.78%       6.148ms     512.363us      14.240us         0.12%      28.480us       2.373us           0 b           0 b       6.00 Kb           0 b            12  \n",
            "                                aten::cudnn_convolution        10.68%       3.318ms        17.44%       5.421ms      90.356us       8.683ms        75.07%       8.683ms     144.713us           0 b           0 b     106.63 Mb     106.63 Mb            60  \n",
            "                                       cudaLaunchKernel         8.56%       2.660ms         8.56%       2.660ms       9.499us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           280  \n",
            "                               aten::upsample_nearest2d         5.18%       1.610ms         5.31%       1.650ms     824.790us      48.735us         0.42%      48.735us      24.368us           0 b           0 b       5.53 Mb       5.53 Mb             2  \n",
            "                                             aten::add_         2.66%     825.428us         4.21%       1.309ms      21.809us       1.088ms         9.40%       1.088ms      18.126us           0 b           0 b           0 b           0 b            60  \n",
            "                                              aten::cat         1.93%     600.913us         3.29%       1.023ms      51.156us     526.041us         4.55%     526.041us      26.302us           0 b           0 b      52.63 Mb      52.63 Mb            20  \n",
            "                                            aten::silu_         1.45%     451.626us         2.82%     875.914us      15.367us     632.917us         5.47%     632.917us      11.104us           0 b           0 b           0 b           0 b            57  \n",
            "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         2.68%     834.374us         2.68%     834.374us      64.183us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            13  \n",
            "                                              aten::mul         1.08%     334.459us         1.71%     530.251us      35.350us      71.328us         0.62%      71.328us       4.755us           0 b           0 b     789.50 Kb     789.50 Kb            15  \n",
            "                                          aten::reshape         0.49%     152.087us         1.38%     430.349us       7.172us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            60  \n",
            "                                             aten::view         0.99%     308.764us         0.99%     308.764us       4.117us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            75  \n",
            "                                            aten::stack         0.10%      30.352us         0.80%     248.422us      82.807us       0.000us         0.00%      18.559us       6.186us           0 b           0 b      66.00 Kb           0 b             3  \n",
            "                                              aten::add         0.50%     156.640us         0.76%     235.117us      23.512us     100.222us         0.87%     100.222us      10.022us           0 b           0 b       9.96 Mb       9.96 Mb            10  \n",
            "                                       aten::contiguous         0.06%      19.715us         0.65%     201.209us      67.070us       0.000us         0.00%     203.485us      67.828us           0 b           0 b       8.57 Mb           0 b             3  \n",
            "                                            aten::clone         0.07%      21.554us         0.58%     181.494us      60.498us       0.000us         0.00%     203.485us      67.828us           0 b           0 b       8.57 Mb           0 b             3  \n",
            "                                         aten::meshgrid         0.41%     127.835us         0.57%     176.033us      58.678us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3  \n",
            "                                           aten::select         0.43%     134.063us         0.55%     171.993us      11.466us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            15  \n",
            "                                        cudaEventRecord         0.55%     171.728us         0.55%     171.728us       2.862us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            60  \n",
            "                                          aten::sigmoid         0.29%      91.257us         0.44%     136.462us      45.487us      67.359us         0.58%      67.359us      22.453us           0 b           0 b       8.55 Mb       8.55 Mb             3  \n",
            "                                              aten::sub         0.30%      92.248us         0.44%     135.972us      45.324us      13.056us         0.11%      13.056us       4.352us           0 b           0 b     197.00 Kb     197.00 Kb             3  \n",
            "                                        cudaMemsetAsync         0.42%     129.246us         0.42%     129.246us       7.603us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            17  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 31.083ms\n",
            "Self CUDA time total: 11.566ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.utils.bottleneck (used for memory and threading issues)"
      ],
      "metadata": {
        "id": "ZEcsGsB0tMz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.utils.bottleneck detect.py --weights yolov5s.pt --source data/images/zidane.jpg --device 0 --save-txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ZlBNsrtZ5R",
        "outputId": "f1cc0ecd-019d-4daa-f30c-313588146954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`bottleneck` is a tool that can be used as an initial step for debugging\n",
            "bottlenecks in your program.\n",
            "\n",
            "It summarizes runs of your script with the Python profiler and PyTorch's\n",
            "autograd profiler. Because your script will be profiled, please ensure that it\n",
            "exits in a finite amount of time.\n",
            "\n",
            "For more complicated uses of the profilers, please see\n",
            "https://docs.python.org/3/library/profile.html and\n",
            "https://pytorch.org/docs/main/autograd.html#profiler for more information.\n",
            "Running environment analysis...\n",
            "Running your script with cProfile\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=yolov5/data/images/zidane.jpg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/1 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 31.6ms\n",
            "Speed: 0.5ms pre-process, 31.6ms inference, 150.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1myolov5/runs/detect/exp7\u001b[0m\n",
            "1 labels saved to yolov5/runs/detect/exp7/labels\n",
            "Running your script with the autograd profiler...\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=yolov5/data/images/zidane.jpg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/1 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 9.8ms\n",
            "Speed: 0.4ms pre-process, 9.8ms inference, 1.8ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1myolov5/runs/detect/exp8\u001b[0m\n",
            "1 labels saved to yolov5/runs/detect/exp8/labels\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/bottleneck/__main__.py:96: FutureWarning: The attribute `use_cuda` will be deprecated soon, please use ``use_device = 'cuda'`` instead.\n",
            "  with profiler.profile(use_cuda=use_cuda) as prof:\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=yolov5/data/images/zidane.jpg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/1 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 18.4ms\n",
            "Speed: 0.6ms pre-process, 18.4ms inference, 3.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1myolov5/runs/detect/exp9\u001b[0m\n",
            "1 labels saved to yolov5/runs/detect/exp9/labels\n",
            "--------------------------------------------------------------------------------\n",
            "  Environment Summary\n",
            "--------------------------------------------------------------------------------\n",
            "PyTorch 2.6.0+cu124 DEBUG compiled w/ CUDA 12.4\n",
            "Running with Python 3.11 and CUDA 12.5.82\n",
            "\n",
            "`pip3 list` truncated output:\n",
            "numpy==2.0.2\n",
            "nvidia-cublas-cu12==12.4.5.8\n",
            "nvidia-cuda-cupti-cu12==12.4.127\n",
            "nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "nvidia-cuda-runtime-cu12==12.4.127\n",
            "nvidia-cudnn-cu12==9.1.0.70\n",
            "nvidia-cufft-cu12==11.2.1.3\n",
            "nvidia-curand-cu12==10.3.5.147\n",
            "nvidia-cusolver-cu12==11.6.1.9\n",
            "nvidia-cusparse-cu12==12.3.1.170\n",
            "nvidia-cusparselt-cu12==0.6.2\n",
            "nvidia-nccl-cu12==2.21.5\n",
            "nvidia-nvjitlink-cu12==12.4.127\n",
            "nvidia-nvtx-cu12==12.4.127\n",
            "nvtx==0.2.11\n",
            "optree==0.14.1\n",
            "pynvjitlink-cu12==0.5.2\n",
            "torch==2.6.0+cu124\n",
            "torchaudio==2.6.0+cu124\n",
            "torchinfo==1.8.0\n",
            "torchsummary==1.5.1\n",
            "torchvision==0.21.0+cu124\n",
            "triton==3.2.0\n",
            "--------------------------------------------------------------------------------\n",
            "  cProfile output\n",
            "--------------------------------------------------------------------------------\n",
            "         6619382 function calls (6514622 primitive calls) in 6.733 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 13914 to 15 due to restriction <15>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "     2765    0.598    0.000    0.598    0.000 {built-in method marshal.loads}\n",
            "      262    0.362    0.001    0.887    0.003 /usr/lib/python3.11/inspect.py:969(getmodule)\n",
            "      180    0.215    0.001    0.215    0.001 {built-in method torch.conv2d}\n",
            "511295/509322    0.175    0.000    0.368    0.000 {built-in method builtins.hasattr}\n",
            "   4416/1    0.167    0.000    6.744    6.744 {built-in method builtins.exec}\n",
            "  181/179    0.128    0.001    0.133    0.001 {built-in method _imp.create_dynamic}\n",
            "   447937    0.118    0.000    0.155    0.000 /usr/lib/python3.11/inspect.py:283(ismodule)\n",
            "6562/6398    0.114    0.000    1.038    0.000 {built-in method builtins.__build_class__}\n",
            "      471    0.113    0.000    0.113    0.000 {method 'to' of 'torch._C.TensorBase' objects}\n",
            "    20918    0.095    0.000    0.095    0.000 {built-in method posix.stat}\n",
            "853953/847632    0.095    0.000    0.103    0.000 {built-in method builtins.isinstance}\n",
            "     7613    0.092    0.000    0.147    0.000 /usr/lib/python3.11/inspect.py:863(cleandoc)\n",
            "    12939    0.091    0.000    0.112    0.000 /usr/local/lib/python3.11/dist-packages/sympy/multipledispatch/conflict.py:43(edge)\n",
            "        1    0.079    0.079    0.079    0.079 {built-in method torch._ops.torchvision.nms}\n",
            "     2825    0.078    0.000    0.078    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "  autograd profiler output (CPU mode)\n",
            "--------------------------------------------------------------------------------\n",
            "        top 15 events sorted by cpu_time_total\n",
            "\n",
            "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::uniform_        33.14%      11.454ms        33.14%      11.454ms      11.454ms             1  \n",
            "         aten::uniform_        12.99%       4.490ms        12.99%       4.490ms       4.490ms             1  \n",
            "         aten::uniform_        11.34%       3.920ms        11.34%       3.920ms       3.920ms             1  \n",
            "         aten::uniform_         9.49%       3.282ms         9.49%       3.282ms       3.282ms             1  \n",
            "         aten::uniform_         8.45%       2.919ms         8.45%       2.919ms       2.919ms             1  \n",
            "               aten::to         0.03%       9.814us         7.63%       2.637ms       2.637ms             1  \n",
            "         aten::_to_copy         0.05%      18.286us         7.60%       2.628ms       2.628ms             1  \n",
            "    aten::empty_strided         7.34%       2.536ms         7.34%       2.536ms       2.536ms             1  \n",
            "         aten::uniform_         5.56%       1.920ms         5.56%       1.920ms       1.920ms             1  \n",
            "         aten::uniform_         4.44%       1.535ms         4.44%       1.535ms       1.535ms             1  \n",
            "         aten::uniform_         4.36%       1.507ms         4.36%       1.507ms       1.507ms             1  \n",
            "               aten::to         0.01%       3.726us         3.00%       1.038ms       1.038ms             1  \n",
            "         aten::_to_copy         0.02%       8.283us         2.99%       1.034ms       1.034ms             1  \n",
            "            aten::copy_         0.04%      14.551us         2.91%       1.006ms       1.006ms             1  \n",
            "        cudaMemcpyAsync         2.74%     946.961us         2.74%     946.961us     946.961us             1  \n",
            "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 34.564ms\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "  autograd profiler output (CUDA mode)\n",
            "--------------------------------------------------------------------------------\n",
            "        top 15 events sorted by cpu_time_total\n",
            "\n",
            "\tBecause the autograd profiler uses the CUDA event API,\n",
            "\tthe CUDA time column reports approximately max(cuda_time, cpu_time).\n",
            "\tPlease ignore this output if your code does not use CUDA.\n",
            "\n",
            "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "      aten::uniform_        33.05%       9.193ms        33.05%       9.193ms       9.193ms       9.208ms        31.86%       9.208ms       9.208ms             1  \n",
            "      aten::uniform_        11.92%       3.315ms        11.92%       3.315ms       3.315ms       3.330ms        11.52%       3.330ms       3.330ms             1  \n",
            "      aten::uniform_        11.75%       3.269ms        11.75%       3.269ms       3.269ms       3.285ms        11.36%       3.285ms       3.285ms             1  \n",
            "      aten::uniform_        11.68%       3.248ms        11.68%       3.248ms       3.248ms       3.257ms        11.27%       3.257ms       3.257ms             1  \n",
            "      aten::uniform_        10.75%       2.991ms        10.75%       2.991ms       2.991ms       3.007ms        10.40%       3.007ms       3.007ms             1  \n",
            "      aten::uniform_         6.49%       1.806ms         6.49%       1.806ms       1.806ms       1.813ms         6.27%       1.813ms       1.813ms             1  \n",
            "      aten::uniform_         5.34%       1.486ms         5.34%       1.486ms       1.486ms       1.493ms         5.17%       1.493ms       1.493ms             1  \n",
            "      aten::uniform_         5.17%       1.439ms         5.17%       1.439ms       1.439ms       1.446ms         5.00%       1.446ms       1.446ms             1  \n",
            "            aten::to         0.02%       6.433us         3.82%       1.064ms       1.064ms      10.000us         0.03%       1.067ms       1.067ms             1  \n",
            "      aten::_to_copy         3.58%     994.578us         3.79%       1.054ms       1.054ms       1.001ms         3.46%       1.057ms       1.057ms             1  \n",
            "            aten::to         0.03%       8.126us         3.72%       1.035ms       1.035ms      11.000us         0.04%       1.037ms       1.037ms             1  \n",
            "      aten::_to_copy         0.07%      18.213us         3.68%       1.024ms       1.024ms      18.000us         0.06%       1.026ms       1.026ms             1  \n",
            "          aten::diag         0.02%       5.942us         3.66%       1.018ms       1.018ms       8.000us         0.03%       1.020ms       1.020ms             1  \n",
            "    aten::diag_embed         0.08%      21.119us         3.63%       1.009ms       1.009ms      25.000us         0.09%       1.012ms       1.012ms             1  \n",
            "         aten::copy_         0.05%      13.980us         3.55%     987.018us     987.018us     993.000us         3.44%     993.000us     993.000us             1  \n",
            "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 27.815ms\n",
            "Self CUDA time total: 28.905ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.utils.bottleneck detect.py --weights yolov5s.pt --source data/images/zidane.jpg --device 0 --save-txt"
      ],
      "metadata": {
        "id": "2R_clXQF6zHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also adding yolov5 to every path is getting tedious. I'll cd into that directory\n"
      ],
      "metadata": {
        "id": "BCq_1WGw31jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jB9jpKz38mW",
        "outputId": "227c9b64-cbdd-430b-cd8f-446aff663acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "9ejWy407xCzK",
        "outputId": "9dd9fc65-198f-40eb-863b-d3a98faa1e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 11 12:42:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Line Profiler for time spent on each pipeline stage"
      ],
      "metadata": {
        "id": "edBz4szU1rJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INstall Line profiler\n",
        "!pip install line_profiler\n",
        "%load_ext line_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIHpKbZt1x5_",
        "outputId": "82c063ea-08bf-49aa-d7f9-6fc2f13e15ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.11/dist-packages (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adds profiling directives to detect.py\n",
        "!sed -i 's/^def run/@profile\\ndef run/' detect.py"
      ],
      "metadata": {
        "id": "hxmOTjrE5dAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kernprof -l detect.py --weights yolov5s.pt --source data/images/zidane.jpg\n",
        "!python -m line_profiler detect.py.lprof"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDXv82kz6WMk",
        "outputId": "0b5278a2-f7e0-4021-9c4f-fb2f8eccfc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images/zidane.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/1 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 30.7ms\n",
            "Speed: 0.5ms pre-process, 30.7ms inference, 135.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "Wrote profile results to detect.py.lprof\n",
            "Inspect results with:\n",
            "python3 -m line_profiler -rmt \"detect.py.lprof\"\n",
            "Timer unit: 1e-06 s\n",
            "\n",
            "Total time: 2.2199 s\n",
            "File: detect.py\n",
            "Function: run at line 69\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "    69                                           @smart_inference_mode()\n",
            "    70                                           @profile\n",
            "    71                                           def run(\n",
            "    72                                               weights=ROOT / \"yolov5s.pt\",  # model path or triton URL\n",
            "    73                                               source=ROOT / \"data/images\",  # file/dir/URL/glob/screen/0(webcam)\n",
            "    74                                               data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n",
            "    75                                               imgsz=(640, 640),  # inference size (height, width)\n",
            "    76                                               conf_thres=0.25,  # confidence threshold\n",
            "    77                                               iou_thres=0.45,  # NMS IOU threshold\n",
            "    78                                               max_det=1000,  # maximum detections per image\n",
            "    79                                               device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
            "    80                                               view_img=False,  # show results\n",
            "    81                                               save_txt=False,  # save results to *.txt\n",
            "    82                                               save_format=0,  # save boxes coordinates in YOLO format or Pascal-VOC format (0 for YOLO and 1 for Pascal-VOC)\n",
            "    83                                               save_csv=False,  # save results in CSV format\n",
            "    84                                               save_conf=False,  # save confidences in --save-txt labels\n",
            "    85                                               save_crop=False,  # save cropped prediction boxes\n",
            "    86                                               nosave=False,  # do not save images/videos\n",
            "    87                                               classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
            "    88                                               agnostic_nms=False,  # class-agnostic NMS\n",
            "    89                                               augment=False,  # augmented inference\n",
            "    90                                               visualize=False,  # visualize features\n",
            "    91                                               update=False,  # update all models\n",
            "    92                                               project=ROOT / \"runs/detect\",  # save results to project/name\n",
            "    93                                               name=\"exp\",  # save results to project/name\n",
            "    94                                               exist_ok=False,  # existing project/name ok, do not increment\n",
            "    95                                               line_thickness=3,  # bounding box thickness (pixels)\n",
            "    96                                               hide_labels=False,  # hide labels\n",
            "    97                                               hide_conf=False,  # hide confidences\n",
            "    98                                               half=False,  # use FP16 half-precision inference\n",
            "    99                                               dnn=False,  # use OpenCV DNN for ONNX inference\n",
            "   100                                               vid_stride=1,  # video frame-rate stride\n",
            "   101                                           ):\n",
            "   102                                               \"\"\"\n",
            "   103                                               Runs YOLOv5 detection inference on various sources like images, videos, directories, streams, etc.\n",
            "   104                                           \n",
            "   105                                               Args:\n",
            "   106                                                   weights (str | Path): Path to the model weights file or a Triton URL. Default is 'yolov5s.pt'.\n",
            "   107                                                   source (str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n",
            "   108                                                       index. Default is 'data/images'.\n",
            "   109                                                   data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n",
            "   110                                                   imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n",
            "   111                                                   conf_thres (float): Confidence threshold for detections. Default is 0.25.\n",
            "   112                                                   iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n",
            "   113                                                   max_det (int): Maximum number of detections per image. Default is 1000.\n",
            "   114                                                   device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n",
            "   115                                                       best available device.\n",
            "   116                                                   view_img (bool): If True, display inference results using OpenCV. Default is False.\n",
            "   117                                                   save_txt (bool): If True, save results in a text file. Default is False.\n",
            "   118                                                   save_csv (bool): If True, save results in a CSV file. Default is False.\n",
            "   119                                                   save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n",
            "   120                                                   save_crop (bool): If True, save cropped prediction boxes. Default is False.\n",
            "   121                                                   nosave (bool): If True, do not save inference images or videos. Default is False.\n",
            "   122                                                   classes (list[int]): List of class indices to filter detections by. Default is None.\n",
            "   123                                                   agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n",
            "   124                                                   augment (bool): If True, use augmented inference. Default is False.\n",
            "   125                                                   visualize (bool): If True, visualize feature maps. Default is False.\n",
            "   126                                                   update (bool): If True, update all models' weights. Default is False.\n",
            "   127                                                   project (str | Path): Directory to save results. Default is 'runs/detect'.\n",
            "   128                                                   name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n",
            "   129                                                   exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n",
            "   130                                                       False.\n",
            "   131                                                   line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n",
            "   132                                                   hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n",
            "   133                                                   hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n",
            "   134                                                   half (bool): If True, use FP16 half-precision inference. Default is False.\n",
            "   135                                                   dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n",
            "   136                                                   vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n",
            "   137                                           \n",
            "   138                                               Returns:\n",
            "   139                                                   None\n",
            "   140                                           \n",
            "   141                                               Examples:\n",
            "   142                                                   ```python\n",
            "   143                                                   from ultralytics import run\n",
            "   144                                           \n",
            "   145                                                   # Run inference on an image\n",
            "   146                                                   run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n",
            "   147                                           \n",
            "   148                                                   # Run inference on a video with specific confidence threshold\n",
            "   149                                                   run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n",
            "   150                                                   ```\n",
            "   151                                               \"\"\"\n",
            "   152         1          1.5      1.5      0.0      source = str(source)\n",
            "   153         1          1.3      1.3      0.0      save_img = not nosave and not source.endswith(\".txt\")  # save inference images\n",
            "   154         1         51.1     51.1      0.0      is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
            "   155         1          1.5      1.5      0.0      is_url = source.lower().startswith((\"rtsp://\", \"rtmp://\", \"http://\", \"https://\"))\n",
            "   156         1          1.6      1.6      0.0      webcam = source.isnumeric() or source.endswith(\".streams\") or (is_url and not is_file)\n",
            "   157         1          0.7      0.7      0.0      screenshot = source.lower().startswith(\"screen\")\n",
            "   158         1          0.2      0.2      0.0      if is_url and is_file:\n",
            "   159                                                   source = check_file(source)  # download\n",
            "   160                                           \n",
            "   161                                               # Directories\n",
            "   162         1        157.3    157.3      0.0      save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
            "   163         1         82.2     82.2      0.0      (save_dir / \"labels\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
            "   164                                           \n",
            "   165                                               # Load model\n",
            "   166         1      51889.0  51889.0      2.3      device = select_device(device)\n",
            "   167         1    1863718.2    2e+06     84.0      model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "   168         1          1.4      1.4      0.0      stride, names, pt = model.stride, model.names, model.pt\n",
            "   169         1         22.2     22.2      0.0      imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
            "   170                                           \n",
            "   171                                               # Dataloader\n",
            "   172         1          0.3      0.3      0.0      bs = 1  # batch_size\n",
            "   173         1          0.3      0.3      0.0      if webcam:\n",
            "   174                                                   view_img = check_imshow(warn=True)\n",
            "   175                                                   dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
            "   176                                                   bs = len(dataset)\n",
            "   177         1          0.2      0.2      0.0      elif screenshot:\n",
            "   178                                                   dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n",
            "   179                                               else:\n",
            "   180         1        231.7    231.7      0.0          dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
            "   181         1          0.7      0.7      0.0      vid_path, vid_writer = [None] * bs, [None] * bs\n",
            "   182                                           \n",
            "   183                                               # Run inference\n",
            "   184         1      52514.5  52514.5      2.4      model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
            "   185         1         30.2     30.2      0.0      seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n",
            "   186         2       8944.0   4472.0      0.4      for path, im, im0s, vid_cap, s in dataset:\n",
            "   187         2        176.0     88.0      0.0          with dt[0]:\n",
            "   188         1        296.8    296.8      0.0              im = torch.from_numpy(im).to(model.device)\n",
            "   189         1         36.6     36.6      0.0              im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
            "   190         1         75.0     75.0      0.0              im /= 255  # 0 - 255 to 0.0 - 1.0\n",
            "   191         1          4.1      4.1      0.0              if len(im.shape) == 3:\n",
            "   192         1          7.7      7.7      0.0                  im = im[None]  # expand for batch dim\n",
            "   193         1          2.2      2.2      0.0              if model.xml and im.shape[0] > 1:\n",
            "   194                                                           ims = torch.chunk(im, im.shape[0], 0)\n",
            "   195                                           \n",
            "   196                                                   # Inference\n",
            "   197         2         78.2     39.1      0.0          with dt[1]:\n",
            "   198         1          0.3      0.3      0.0              visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
            "   199         1          0.6      0.6      0.0              if model.xml and im.shape[0] > 1:\n",
            "   200                                                           pred = None\n",
            "   201                                                           for image in ims:\n",
            "   202                                                               if pred is None:\n",
            "   203                                                                   pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n",
            "   204                                                               else:\n",
            "   205                                                                   pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n",
            "   206                                                           pred = [pred, None]\n",
            "   207                                                       else:\n",
            "   208         1      30667.5  30667.5      1.4                  pred = model(im, augment=augment, visualize=visualize)\n",
            "   209                                                   # NMS\n",
            "   210         2        140.4     70.2      0.0          with dt[2]:\n",
            "   211         1     134957.2 134957.2      6.1              pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
            "   212                                           \n",
            "   213                                                   # Second-stage classifier (optional)\n",
            "   214                                                   # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
            "   215                                           \n",
            "   216                                                   # Define the path for the CSV file\n",
            "   217         1         53.7     53.7      0.0          csv_path = save_dir / \"predictions.csv\"\n",
            "   218                                           \n",
            "   219                                                   # Create or append to the CSV file\n",
            "   220         1          2.0      2.0      0.0          def write_to_csv(image_name, prediction, confidence):\n",
            "   221                                                       \"\"\"Writes prediction data for an image to a CSV file, appending if the file exists.\"\"\"\n",
            "   222                                                       data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n",
            "   223                                                       file_exists = os.path.isfile(csv_path)\n",
            "   224                                                       with open(csv_path, mode=\"a\", newline=\"\") as f:\n",
            "   225                                                           writer = csv.DictWriter(f, fieldnames=data.keys())\n",
            "   226                                                           if not file_exists:\n",
            "   227                                                               writer.writeheader()\n",
            "   228                                                           writer.writerow(data)\n",
            "   229                                           \n",
            "   230                                                   # Process predictions\n",
            "   231         2          5.7      2.9      0.0          for i, det in enumerate(pred):  # per image\n",
            "   232         1          0.9      0.9      0.0              seen += 1\n",
            "   233         1          0.4      0.4      0.0              if webcam:  # batch_size >= 1\n",
            "   234                                                           p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
            "   235                                                           s += f\"{i}: \"\n",
            "   236                                                       else:\n",
            "   237         1       1790.7   1790.7      0.1                  p, im0, frame = path, im0s.copy(), getattr(dataset, \"frame\", 0)\n",
            "   238                                           \n",
            "   239         1         82.0     82.0      0.0              p = Path(p)  # to Path\n",
            "   240         1         38.2     38.2      0.0              save_path = str(save_dir / p.name)  # im.jpg\n",
            "   241         1         30.0     30.0      0.0              txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n",
            "   242         1         24.0     24.0      0.0              s += \"{:g}x{:g} \".format(*im.shape[2:])  # print string\n",
            "   243         1        137.6    137.6      0.0              gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
            "   244         1          0.3      0.3      0.0              imc = im0.copy() if save_crop else im0  # for save_crop\n",
            "   245         1        456.3    456.3      0.0              annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
            "   246         1         20.8     20.8      0.0              if len(det):\n",
            "   247                                                           # Rescale boxes from img_size to im0 size\n",
            "   248         1      23778.1  23778.1      1.1                  det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
            "   249                                           \n",
            "   250                                                           # Print results\n",
            "   251         3      19740.6   6580.2      0.9                  for c in det[:, 5].unique():\n",
            "   252         2      21836.9  10918.5      1.0                      n = (det[:, 5] == c).sum()  # detections per class\n",
            "   253         2        467.8    233.9      0.0                      s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
            "   254                                           \n",
            "   255                                                           # Write results\n",
            "   256         5        243.7     48.7      0.0                  for *xyxy, conf, cls in reversed(det):\n",
            "   257         4         89.6     22.4      0.0                      c = int(cls)  # integer class\n",
            "   258         4          3.3      0.8      0.0                      label = names[c] if hide_conf else f\"{names[c]}\"\n",
            "   259         4         60.8     15.2      0.0                      confidence = float(conf)\n",
            "   260         4         13.7      3.4      0.0                      confidence_str = f\"{confidence:.2f}\"\n",
            "   261                                           \n",
            "   262         4          1.8      0.4      0.0                      if save_csv:\n",
            "   263                                                                   write_to_csv(p.name, label, confidence_str)\n",
            "   264                                           \n",
            "   265         4          1.8      0.4      0.0                      if save_txt:  # Write to file\n",
            "   266                                                                   if save_format == 0:\n",
            "   267                                                                       coords = (\n",
            "   268                                                                           (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
            "   269                                                                       )  # normalized xywh\n",
            "   270                                                                   else:\n",
            "   271                                                                       coords = (torch.tensor(xyxy).view(1, 4) / gn).view(-1).tolist()  # xyxy\n",
            "   272                                                                   line = (cls, *coords, conf) if save_conf else (cls, *coords)  # label format\n",
            "   273                                                                   with open(f\"{txt_path}.txt\", \"a\") as f:\n",
            "   274                                                                       f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
            "   275                                           \n",
            "   276         4          1.1      0.3      0.0                      if save_img or save_crop or view_img:  # Add bbox to image\n",
            "   277         4         42.6     10.7      0.0                          c = int(cls)  # integer class\n",
            "   278         4         72.8     18.2      0.0                          label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n",
            "   279         4       1954.1    488.5      0.1                          annotator.box_label(xyxy, label, color=colors(c, True))\n",
            "   280         4          1.6      0.4      0.0                      if save_crop:\n",
            "   281                                                                   save_one_box(xyxy, imc, file=save_dir / \"crops\" / names[c] / f\"{p.stem}.jpg\", BGR=True)\n",
            "   282                                           \n",
            "   283                                                       # Stream results\n",
            "   284         1          5.8      5.8      0.0              im0 = annotator.result()\n",
            "   285         1          0.2      0.2      0.0              if view_img:\n",
            "   286                                                           if platform.system() == \"Linux\" and p not in windows:\n",
            "   287                                                               windows.append(p)\n",
            "   288                                                               cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
            "   289                                                               cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
            "   290                                                           cv2.imshow(str(p), im0)\n",
            "   291                                                           cv2.waitKey(1)  # 1 millisecond\n",
            "   292                                           \n",
            "   293                                                       # Save results (image with detections)\n",
            "   294         1          0.2      0.2      0.0              if save_img:\n",
            "   295         1          1.5      1.5      0.0                  if dataset.mode == \"image\":\n",
            "   296         1       4442.3   4442.3      0.2                      cv2.imwrite(save_path, im0)\n",
            "   297                                                           else:  # 'video' or 'stream'\n",
            "   298                                                               if vid_path[i] != save_path:  # new video\n",
            "   299                                                                   vid_path[i] = save_path\n",
            "   300                                                                   if isinstance(vid_writer[i], cv2.VideoWriter):\n",
            "   301                                                                       vid_writer[i].release()  # release previous video writer\n",
            "   302                                                                   if vid_cap:  # video\n",
            "   303                                                                       fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
            "   304                                                                       w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
            "   305                                                                       h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
            "   306                                                                   else:  # stream\n",
            "   307                                                                       fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
            "   308                                                                   save_path = str(Path(save_path).with_suffix(\".mp4\"))  # force *.mp4 suffix on results videos\n",
            "   309                                                                   vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
            "   310                                                               vid_writer[i].write(im0)\n",
            "   311                                           \n",
            "   312                                                   # Print time (inference-only)\n",
            "   313         1        213.6    213.6      0.0          LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1e3:.1f}ms\")\n",
            "   314                                           \n",
            "   315                                               # Print results\n",
            "   316         1         11.2     11.2      0.0      t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n",
            "   317         1         88.9     88.9      0.0      LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}\" % t)\n",
            "   318         1          0.4      0.4      0.0      if save_txt or save_img:\n",
            "   319         1          0.4      0.4      0.0          s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else \"\"\n",
            "   320         1         90.8     90.8      0.0          LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
            "   321         1          0.3      0.3      0.0      if update:\n",
            "   322                                                   strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, loading the model is taking up too much time. Modify detect.py directly to print the times for each stage. The modification was done on my device and the output obtained is displayed in the cell below.\n",
        "\n",
        "# NOTE:\n",
        "Kindly look at the saved output of the cell below before running it as it was obtained after a modification to a file on my device. Running it again without changing the `detect.py` file accordinly will not print the time percentage details..\n"
      ],
      "metadata": {
        "id": "hojNG_tj_Nwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !kernprof -l detect.py --weights yolov5s.pt --source data/images/zidane.jpg\n",
        "# !python -m line_profiler detect.py.lprof\n",
        "!python3 detect.py --weights yolov5s.pt --source data/images/zidane.jpg --save-txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IQRWhTj_YVY",
        "outputId": "921f20de-e875-422f-a19a-d999de8f5ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images/zidane.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/1 /content/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 31.5ms\n",
            "\n",
            "--- Pipeline Breakdown (after model load) ---\n",
            "Pre-processing:  0.20%\n",
            "Inference:       14.51%\n",
            "Post-processing: 85.29%\n",
            "Speed: 0.4ms pre-process, 31.5ms inference, 185.2ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n",
            "1 labels saved to runs/detect/exp2/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4"
      ],
      "metadata": {
        "id": "YN0FqIAimsuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(model):\n",
        "  # Run detection with timing\n",
        "  model = model.strip()\n",
        "  if not os.path.exists(model):\n",
        "        raise FileNotFoundError(f\"Model file {model} not found\")\n",
        "  start_time = time.time()\n",
        "  cmd = f\"python3 detect.py --weights {model} --source coco_subset --device {device} --save-txt\"\n",
        "  result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "  output = result.stderr\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate FPS\n",
        "  fps = 100 / (end_time - start_time)\n",
        "  # print(output)\n",
        "  # Calculate latency - search for the string in the given format from the output of the command detect.py\n",
        "  match = re.search(r\"Speed:\\s+([\\d.]+)ms pre-process,\\s+([\\d.]+)ms inference,\\s+([\\d.]+)ms NMS\", output)\n",
        "  if match:\n",
        "      pre = float(match.group(1))\n",
        "      infer = float(match.group(2))\n",
        "      nms = float(match.group(3))\n",
        "      total_latency = round(pre + infer + nms, 2)\n",
        "      return {\n",
        "          \"model\": model,\n",
        "          \"latency\": total_latency,\n",
        "          \"fps\": fps\n",
        "      }\n",
        "  else:\n",
        "    print(\"Something wrong\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9TwzAgFoqiz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 1: Change to FP16"
      ],
      "metadata": {
        "id": "TQuOPNCQnxU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FP32 s\n",
        "print(benchmark('yolov5s.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guu8HK4UrtRm",
        "outputId": "9df7b945-7101-4b27-f6b0-74dc14496c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5s.pt', 'latency': 16.0, 'fps': 9.979759735068718}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify Source code to use FP16\n",
        "\n",
        "#FP16 s\n",
        "print(benchmark('yolov5s.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7NauqhfmsSz",
        "outputId": "a6ea008a-2f35-498c-b4fb-7410c12be97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5s.pt', 'latency': 15.0, 'fps': 10.27355132176902}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FP16 X\n",
        "print(benchmark('yolov5x.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F7D-Ke3v_Bc",
        "outputId": "2ac5b00f-012d-48e3-c22a-54a73bce3967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x.pt', 'latency': 44.0, 'fps': 7.551323448153921}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FP32 X\n",
        "print(benchmark('yolov5x.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T42w3au5vhUn",
        "outputId": "b0bec0ae-95ac-4abf-9868-03495aab8a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x.pt', 'latency': 27.6, 'fps': 8.411387551539327}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 2: Batch Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "bVM2h0GFyasu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov5s Batch size: 1\n",
        "print(benchmark('yolov5s.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikfZvVCg5y46",
        "outputId": "55da0a00-2eda-4971-9b91-c48cf629d376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5s.pt', 'latency': 13.6, 'fps': 11.033102854472979}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify source code\n",
        "# yolov5s Batch Size: 32\n",
        "print(benchmark('yolov5s.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tWLOHNY7jjQ",
        "outputId": "4807075a-557f-44f1-f565-028a1ac94302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5s.pt', 'latency': 13.5, 'fps': 11.21276663544928}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov5s Batch size 100\n",
        "print(benchmark('yolov5s.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKU5Ztpr8v0v",
        "outputId": "249fbf2a-bfd7-4b90-e21b-66bca4c4e691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5s.pt', 'latency': 13.4, 'fps': 11.534726277463598}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov5x Batch size 1\n",
        "print(benchmark('yolov5x.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTaVR2n_8EO3",
        "outputId": "92dc80ba-7c7f-405d-da67-7b5b43e0bfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x.pt', 'latency': 41.2, 'fps': 7.6264871867664255}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov5x Batch size 32\n",
        "print(benchmark('yolov5x.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKVnbS0974JN",
        "outputId": "01976e5f-141a-44f4-c012-aab8b388ffb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x.pt', 'latency': 41.0, 'fps': 7.659538643945381}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov5x Batch size 100\n",
        "print(benchmark('yolov5x'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u77skE6Q8wz0",
        "outputId": "9a558071-bd6f-403c-b175-721b07940a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x', 'latency': 44.0, 'fps': 7.343453094493132}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom script to bypass detect.py and directly measure inference time"
      ],
      "metadata": {
        "id": "ygJGwiQg7D-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load model once and reuse\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x', device=device)\n",
        "model.eval().half()\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load all image paths once (modify path if needed)\n",
        "image_dir = 'coco_subset'  # Change to your folder\n",
        "image_paths = list(Path(image_dir).glob('*.jpg'))\n",
        "\n",
        "# Define batched benchmark function\n",
        "def benchmark_batch(batch_size):\n",
        "    if batch_size > len(image_paths):\n",
        "        print(f\"Only {len(image_paths)} images available, reducing batch size.\")\n",
        "        batch_size = len(image_paths)\n",
        "\n",
        "    imgs = []\n",
        "    for path in image_paths[:batch_size]:\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = transform(img)\n",
        "        imgs.append(img)\n",
        "\n",
        "    batch = torch.stack(imgs).to('cuda').half()\n",
        "\n",
        "    # Inference and timing\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(batch)\n",
        "    end = time.time()\n",
        "\n",
        "    latency = (end - start) * 1000  # ms\n",
        "    fps = batch_size / (end - start)\n",
        "\n",
        "    return {\n",
        "        'batch_size': batch_size,\n",
        "        'latency (ms)': round(latency, 2),\n",
        "        'fps': round(fps, 2)\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g64Ixj7X0V87",
        "outputId": "6d9e17b4-c7d0-4dd2-95fb-ffa43d60be21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov5s\n",
        "print(benchmark_batch(1))\n",
        "print(benchmark_batch(16))\n",
        "print(benchmark_batch(32))\n",
        "print(benchmark_batch(64))\n",
        "print(benchmark_batch(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoC1sGYr0-Ry",
        "outputId": "f3d30575-5d6d-40c7-d16f-b0f07ee7fed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 1, 'latency (ms)': 8.96, 'fps': 111.57}\n",
            "{'batch_size': 16, 'latency (ms)': 7.75, 'fps': 2065.27}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 32, 'latency (ms)': 7.85, 'fps': 4074.12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 64, 'latency (ms)': 8.2, 'fps': 7801.54}\n",
            "{'batch_size': 100, 'latency (ms)': 8.12, 'fps': 12320.61}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the benchmark_batch() to change the model from yolov5s to yolov5x\n",
        "\n",
        "# yolov5x\n",
        "print(benchmark_batch(1))\n",
        "print(benchmark_batch(16))\n",
        "print(benchmark_batch(32))\n",
        "print(benchmark_batch(64))\n",
        "print(benchmark_batch(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K6Stsrx2LQ4",
        "outputId": "7c967c58-76db-490b-affa-4f3977d020d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 1, 'latency (ms)': 15.26, 'fps': 65.55}\n",
            "{'batch_size': 16, 'latency (ms)': 15.27, 'fps': 1047.74}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 32, 'latency (ms)': 13.74, 'fps': 2328.23}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 64, 'latency (ms)': 13.84, 'fps': 4623.73}\n",
            "{'batch_size': 100, 'latency (ms)': 13.77, 'fps': 7261.48}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:879: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 3: ONNX/TensorRT"
      ],
      "metadata": {
        "id": "FYVlZgfBYZOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "8_kG5Ly9AIKO",
        "outputId": "c6b620ec-e091-4124-dcfc-7d2e9e96aa0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Using cached onnxruntime_gpu-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Using cached onnxruntime_gpu-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "Installing collected packages: onnxruntime-gpu\n",
            "Successfully installed onnxruntime-gpu-1.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "onnxruntime"
                ]
              },
              "id": "25cd7f4dd36445abbfb5c652cc71e0bb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_onnx(model):\n",
        "  # Run detection with timing\n",
        "  model = model.strip()\n",
        "  # if not os.path.exists(model):\n",
        "  #       raise FileNotFoundError(f\"Model file {model} not found\")\n",
        "  start_time = time.time()\n",
        "  cmd = f\"python3 detect.py --weights {model} --source coco_subset --device {device} --half --save-txt\"\n",
        "  result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "  output = result.stderr\n",
        "  end_time = time.time()\n",
        "  print(output)\n",
        "  # Calculate FPS\n",
        "  fps = 100 / (end_time - start_time)\n",
        "  # print(output)\n",
        "  # Calculate latency - search for the string in the given format from the output of the command detect.py\n",
        "  match = re.search(r\"Speed:\\s+([\\d.]+)ms pre-process,\\s+([\\d.]+)ms inference,\\s+([\\d.]+)ms NMS\", output)\n",
        "  if match:\n",
        "      pre = float(match.group(1))\n",
        "      infer = float(match.group(2))\n",
        "      nms = float(match.group(3))\n",
        "      total_latency = round(pre + infer + nms, 2)\n",
        "      return {\n",
        "          \"model\": model,\n",
        "          \"latency\": total_latency,\n",
        "          \"fps\": fps\n",
        "      }\n",
        "  else:\n",
        "    print(\"Something wrong\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0wlFc2_o7yb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify detect.py here to include onnx\n",
        "# YOLOV5s\n",
        "# With onnx\n",
        "print(benchmark_onnx('yolov5s.onnx'))"
      ],
      "metadata": {
        "id": "orHL5epTh-ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOV5s\n",
        "# Without ONNX\n",
        "print(benchmark('yolov5s.onnx'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_qB0jZxigu_",
        "outputId": "9f2561b8-2312-4fbd-9b76-9280ffa129c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5s.onnx', 'latency': 228.8, 'fps': 3.2865758685683084}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOV5x\n",
        "# Without ONNX\n",
        "print(benchmark('yolov5x.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcWxy706i2Xd",
        "outputId": "448eb82f-8f27-46b8-ec19-d88b9fc3092f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x.pt', 'latency': 41.7, 'fps': 7.83334640975327}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOV5x\n",
        "# With ONNX\n",
        "print(benchmark('yolov5x.onnx'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0c_w7eHjPNH",
        "outputId": "7e87c6d9-95dc-48df-806f-dfb1571f0f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'yolov5x.onnx', 'latency': 2129.1, 'fps': 0.44623302141253224}\n"
          ]
        }
      ]
    }
  ]
}